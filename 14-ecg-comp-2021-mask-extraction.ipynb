{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebbf5006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:26.798709Z",
     "iopub.status.busy": "2025-05-27T09:15:26.798469Z",
     "iopub.status.idle": "2025-05-27T09:15:33.179321Z",
     "shell.execute_reply": "2025-05-27T09:15:33.178291Z"
    },
    "papermill": {
     "duration": 6.389362,
     "end_time": "2025-05-27T09:15:33.181008",
     "exception": false,
     "start_time": "2025-05-27T09:15:26.791646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\r\n",
      "Collecting pip\r\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pip\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 24.1.2\r\n",
      "    Uninstalling pip-24.1.2:\r\n",
      "      Successfully uninstalled pip-24.1.2\r\n",
      "Successfully installed pip-25.1.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7e4b69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:33.195106Z",
     "iopub.status.busy": "2025-05-27T09:15:33.194504Z",
     "iopub.status.idle": "2025-05-27T09:15:38.701571Z",
     "shell.execute_reply": "2025-05-27T09:15:38.700797Z"
    },
    "papermill": {
     "duration": 5.51552,
     "end_time": "2025-05-27T09:15:38.703049",
     "exception": false,
     "start_time": "2025-05-27T09:15:33.187529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.6.1\r\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (1.15.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (3.6.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.6.1) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.6.1) (2022.1.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.6.1) (2024.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.6.1) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.6.1) (2024.2.0)\r\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.6.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.6.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c3fdee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:38.718100Z",
     "iopub.status.busy": "2025-05-27T09:15:38.717836Z",
     "iopub.status.idle": "2025-05-27T09:15:41.687660Z",
     "shell.execute_reply": "2025-05-27T09:15:41.686769Z"
    },
    "papermill": {
     "duration": 2.978416,
     "end_time": "2025-05-27T09:15:41.689061",
     "exception": false,
     "start_time": "2025-05-27T09:15:38.710645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85d951a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:41.704122Z",
     "iopub.status.busy": "2025-05-27T09:15:41.703725Z",
     "iopub.status.idle": "2025-05-27T09:15:45.843727Z",
     "shell.execute_reply": "2025-05-27T09:15:45.842745Z"
    },
    "id": "roZgmsPyFKjB",
    "outputId": "7ace9a87-c359-42a4-e43c-bc2a5afe7715",
    "papermill": {
     "duration": 4.149345,
     "end_time": "2025-05-27T09:15:45.845595",
     "exception": false,
     "start_time": "2025-05-27T09:15:41.696250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install wfdb --quiet\n",
    "#!pip install keras-tuner --quiet\n",
    "#!pip install resnet1d\n",
    "!pip install tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73def121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:45.858711Z",
     "iopub.status.busy": "2025-05-27T09:15:45.858460Z",
     "iopub.status.idle": "2025-05-27T09:15:46.106234Z",
     "shell.execute_reply": "2025-05-27T09:15:46.105659Z"
    },
    "id": "L5ge4LwBPcr6",
    "papermill": {
     "duration": 0.255714,
     "end_time": "2025-05-27T09:15:46.107501",
     "exception": false,
     "start_time": "2025-05-27T09:15:45.851787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wfdb\n",
    "from scipy.signal import butter, filtfilt\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588900ea",
   "metadata": {
    "id": "ZTMmDNHCfhhu",
    "papermill": {
     "duration": 0.005416,
     "end_time": "2025-05-27T09:15:46.118840",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.113424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcab235",
   "metadata": {
    "id": "jvXrP01oftqo",
    "papermill": {
     "duration": 0.005283,
     "end_time": "2025-05-27T09:15:46.129626",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.124343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## preprocess Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34201bc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.141561Z",
     "iopub.status.busy": "2025-05-27T09:15:46.141222Z",
     "iopub.status.idle": "2025-05-27T09:15:46.147295Z",
     "shell.execute_reply": "2025-05-27T09:15:46.146755Z"
    },
    "id": "JNp2geUV6p_G",
    "papermill": {
     "duration": 0.013215,
     "end_time": "2025-05-27T09:15:46.148347",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.135132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "import numpy as np\n",
    "def bandpass_filter(signal, fs=250, lowcut=0.5,  highcut=15.0,  order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "    \n",
    "def smooth_signal(data, window_size=5):\n",
    "    window = np.ones(window_size) / window_size\n",
    "    smoothed = np.convolve(data, window, mode='same')\n",
    "    return smoothed\n",
    "def normalize_signal(data):\n",
    "    return (data - np.mean(data)) / np.std(data)\n",
    "    \n",
    "\n",
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "def resample_signal(signal, original_fs, target_fs):\n",
    "    num_samples = int(len(signal) * target_fs / original_fs)\n",
    "    resampled_signal = resample(signal, num_samples)\n",
    "    return resampled_signal\n",
    "def adjust_annotations(samples, original_fs, target_fs):\n",
    "    scale = target_fs / original_fs\n",
    "    if isinstance(samples, (list, np.ndarray)):\n",
    "        return (np.array(samples) * scale).astype(int)\n",
    "    else:\n",
    "        return int(samples * scale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0027d",
   "metadata": {
    "id": "ZW-HQbGIN86a",
    "papermill": {
     "duration": 0.005356,
     "end_time": "2025-05-27T09:15:46.159170",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.153814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26682db1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.170815Z",
     "iopub.status.busy": "2025-05-27T09:15:46.170615Z",
     "iopub.status.idle": "2025-05-27T09:15:46.176005Z",
     "shell.execute_reply": "2025-05-27T09:15:46.175467Z"
    },
    "id": "Fd5btn-3IGM1",
    "papermill": {
     "duration": 0.012299,
     "end_time": "2025-05-27T09:15:46.176984",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.164685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_full_ecg(signal, model, fs=250, window_size=240):\n",
    "    model.eval()\n",
    "    signal = signal[:len(signal) - (len(signal) % window_size)]  # crop to multiple of window_size\n",
    "    signal = normalize_signal(smooth_signal(bandpass_filter(signal, fs=fs)))\n",
    "    if fs != 250:\n",
    "        signal = resample_signal(signal, original_fs=fs, target_fs=250)\n",
    "        fs = fs\n",
    "    signal = signal.reshape(1, 1, -1)  # (1, 1, L)\n",
    "\n",
    "    segments = []\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(0, signal.shape[2] - window_size + 1, window_size):\n",
    "        window = signal[:, :, i:i+window_size]\n",
    "        segments.append(window)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for window in segments:\n",
    "            window = torch.tensor(window, dtype=torch.float32).to(device)\n",
    "            output = model(window)  # (1, C, L)\n",
    "            pred = torch.argmax(output, dim=1).cpu().numpy()[0]  # (L,)\n",
    "            predictions.append(pred)\n",
    "\n",
    "    full_prediction = np.concatenate(predictions)\n",
    "    return full_prediction, segments, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "058b2e6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.188756Z",
     "iopub.status.busy": "2025-05-27T09:15:46.188543Z",
     "iopub.status.idle": "2025-05-27T09:15:46.193397Z",
     "shell.execute_reply": "2025-05-27T09:15:46.192871Z"
    },
    "id": "hqpbvUexIbT1",
    "papermill": {
     "duration": 0.011878,
     "end_time": "2025-05-27T09:15:46.194393",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.182515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wave_boundaries(predicted_labels):\n",
    "    wave_types = {1: 'P', 2: 'QRS', 3: 'T'}\n",
    "    wave_boundaries = {k: [] for k in wave_types.values()}\n",
    "\n",
    "    current_class = None\n",
    "    start = None\n",
    "\n",
    "    for i, label in enumerate(predicted_labels):\n",
    "        if label in wave_types:\n",
    "            if current_class is None:\n",
    "                current_class = label\n",
    "                start = i\n",
    "            elif label != current_class:\n",
    "                wave_boundaries[wave_types[current_class]].append((start, i-1))\n",
    "                current_class = label\n",
    "                start = i\n",
    "        else:\n",
    "\n",
    "            if current_class is not None:\n",
    "                wave_boundaries[wave_types[current_class]].append((start, i-1))\n",
    "                current_class = None\n",
    "                start = None\n",
    "\n",
    "    # Handle last segment\n",
    "    if current_class is not None:\n",
    "        wave_boundaries[wave_types[current_class]].append((start, len(predicted_labels)-1))\n",
    "\n",
    "    return wave_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7da72158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.206216Z",
     "iopub.status.busy": "2025-05-27T09:15:46.206014Z",
     "iopub.status.idle": "2025-05-27T09:15:46.212239Z",
     "shell.execute_reply": "2025-05-27T09:15:46.211717Z"
    },
    "id": "KBxpb7yjQzg7",
    "papermill": {
     "duration": 0.013276,
     "end_time": "2025-05-27T09:15:46.213164",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.199888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_boundaries(signal, boundaries, signal_name, model_name, fs=250, zoom=(0, None)):\n",
    "    time = np.arange(len(signal)) / fs  # Convert indices to time (seconds)\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.plot(time, signal, color='black', label='ECG Signal', linewidth=1)\n",
    "\n",
    "    wave_colors = {'P': 'blue', 'QRS': 'red', 'T': 'green'}\n",
    "    added_labels = set()\n",
    "\n",
    "    # Convert zoom limits to sample indices\n",
    "    zoom_start_idx = int(zoom[0] * fs)\n",
    "    zoom_end_idx = int(zoom[1] * fs) if zoom[1] is not None else len(signal)\n",
    "\n",
    "    for wave_type, regions in boundaries.items():\n",
    "        for start, end in regions:\n",
    "            # Only plot if within zoom range\n",
    "            if start >= zoom_start_idx and end <= zoom_end_idx:\n",
    "                label = f'Predicted {wave_type}' if wave_type not in added_labels else None\n",
    "                plt.fill_between(\n",
    "                    time[start:end],\n",
    "                    -1, 1,\n",
    "                    color=wave_colors.get(wave_type, 'gray'),\n",
    "                    alpha=0.3,\n",
    "                    label=label\n",
    "                )\n",
    "                added_labels.add(wave_type)\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"signal: {signal_name}, model: {model_name}\", fontsize=16)\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Apply zoom\n",
    "    plt.xlim(zoom[0], zoom[1] if zoom[1] is not None else time[-1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4637a5d",
   "metadata": {
    "id": "UCgs5n5KgSse",
    "papermill": {
     "duration": 0.005451,
     "end_time": "2025-05-27T09:15:46.224039",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.218588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Post-process Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6138a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.236146Z",
     "iopub.status.busy": "2025-05-27T09:15:46.235875Z",
     "iopub.status.idle": "2025-05-27T09:15:46.240511Z",
     "shell.execute_reply": "2025-05-27T09:15:46.240007Z"
    },
    "id": "kmS2kirJ8Jvn",
    "outputId": "741640e0-0117-4dfe-af6f-63ba631b3b53",
    "papermill": {
     "duration": 0.011909,
     "end_time": "2025-05-27T09:15:46.241537",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.229628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_uncomplete_first_last_wave(predicted):\n",
    "    start = predicted[0]\n",
    "    end = predicted[-1]\n",
    "    if start != 0:\n",
    "      i=0\n",
    "      while i < len(predicted) and predicted[i]==start:\n",
    "        i+=1\n",
    "      predicted[:i]=0\n",
    "    if end != 0:\n",
    "      i=len(predicted)-1\n",
    "      while i > -1 and predicted[i]==end:\n",
    "        i-=1\n",
    "      predicted[i+1:]=0\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b1bc8b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.253545Z",
     "iopub.status.busy": "2025-05-27T09:15:46.253342Z",
     "iopub.status.idle": "2025-05-27T09:15:46.257389Z",
     "shell.execute_reply": "2025-05-27T09:15:46.256882Z"
    },
    "id": "7e5D4ca1e68y",
    "outputId": "2235cdc7-183d-42ef-f842-be84ca20ebfe",
    "papermill": {
     "duration": 0.011268,
     "end_time": "2025-05-27T09:15:46.258436",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.247168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def merge_close_waves(predicted, max_gap=10):\n",
    "  predicted = predicted.copy()\n",
    "  for target_class in [1,2,3]:\n",
    "    indices = np.where(predicted == target_class)[0]\n",
    "\n",
    "    if len(indices) < 2:\n",
    "        return predicted  \n",
    "\n",
    "    for i in range(len(indices) - 1):\n",
    "        current = indices[i]\n",
    "        next_ = indices[i + 1]\n",
    "        if 0 < next_ - current - 1 < max_gap:\n",
    "            predicted[current:next_ + 1] = target_class\n",
    "\n",
    "\n",
    "\n",
    "  return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46448001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.270308Z",
     "iopub.status.busy": "2025-05-27T09:15:46.270117Z",
     "iopub.status.idle": "2025-05-27T09:15:46.275872Z",
     "shell.execute_reply": "2025-05-27T09:15:46.275398Z"
    },
    "id": "TGZYAxZz8Ftk",
    "outputId": "af8d22cd-fd2c-4ff7-d957-9affa4a8d681",
    "papermill": {
     "duration": 0.012811,
     "end_time": "2025-05-27T09:15:46.276884",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.264073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_irrelevant_waves(predicted,start_search=2,end_search=5):\n",
    "    start=0\n",
    "    # Find the first P that has a QRS after it\n",
    "    if 1 in predicted[:start_search*250] :\n",
    "      for i in range(len(predicted)-1):\n",
    "          start = i\n",
    "\n",
    "          if predicted[i] == 1:\n",
    "              start = i\n",
    "              \n",
    "              while i < len(predicted)-1 and predicted[i] == 1:\n",
    "                i += 1\n",
    "              # if it's not 0 or 2 break\n",
    "              if predicted[i] == 3:\n",
    "                continue\n",
    "              # skip background if it exist\n",
    "              if predicted[i] == 0:\n",
    "                while i < len(predicted)-1 and predicted[i] == 0:\n",
    "                  i += 1\n",
    "              # if it's not qrs continue to next p\n",
    "              if predicted[i] != 2:\n",
    "                continue\n",
    "              else:\n",
    "                break\n",
    "      predicted[:start] = 0\n",
    "\n",
    "\n",
    "    # remove after last T\n",
    "    if 3 in predicted[-end_search*250:]:\n",
    "      \n",
    "      end = predicted[-1]\n",
    "      for i in range((len(predicted) - 1), -1, -1):\n",
    "          end = i\n",
    "\n",
    "          if predicted[i] == 3:\n",
    "              end = i\n",
    "              while i > 0 and predicted[i] == 3:\n",
    "                i -= 1\n",
    "              # if it's not 0 or 2 break\n",
    "              if predicted[i] == 1:\n",
    "                continue\n",
    "              # skip background if it exist\n",
    "              if predicted[i] == 0:\n",
    "                while i > 0 and predicted[i] == 0:\n",
    "                  i -= 1\n",
    "              # if it's not qrs continue to next t\n",
    "              if predicted[i] != 2:\n",
    "                continue\n",
    "              else:\n",
    "                break\n",
    "      predicted[end+1:] = 0\n",
    "\n",
    "    return (predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "480f090e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.288845Z",
     "iopub.status.busy": "2025-05-27T09:15:46.288655Z",
     "iopub.status.idle": "2025-05-27T09:15:46.294168Z",
     "shell.execute_reply": "2025-05-27T09:15:46.293678Z"
    },
    "id": "CSwwTMCAcJ7h",
    "outputId": "9c0e8129-f653-4b9f-8e4c-f1732f45e059",
    "papermill": {
     "duration": 0.012667,
     "end_time": "2025-05-27T09:15:46.295239",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.282572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_repeated_waves(predicted):\n",
    "    cleaned = predicted.copy()\n",
    "    for target_class in [1,2,3]:\n",
    "      segments = []\n",
    "      in_segment = False\n",
    "      start = 0\n",
    "\n",
    "      # Step 1: Collect all segments of the target class\n",
    "      for i, val in enumerate(predicted):\n",
    "          if val == target_class and not in_segment:\n",
    "              in_segment = True\n",
    "              start = i\n",
    "          elif val != target_class and in_segment:\n",
    "              in_segment = False\n",
    "              segments.append((start, i - 1))\n",
    "      if in_segment:\n",
    "          segments.append((start, len(predicted) - 1))\n",
    "\n",
    "      # Step 2: Check for pairs of segments with only background (0) in between\n",
    "      i = 0\n",
    "      while i < len(segments) - 1:\n",
    "          s1, e1 = segments[i]\n",
    "          s2, e2 = segments[i + 1]\n",
    "          between = cleaned[e1 + 1:s2]\n",
    "\n",
    "          if np.all(between == 0):  # Only background between them\n",
    "              len1 = e1 - s1 + 1\n",
    "              len2 = e2 - s2 + 1\n",
    "\n",
    "              # Remove the shorter one\n",
    "              if len1 < len2:\n",
    "                  cleaned[s1:e1 + 1] = 0\n",
    "              else:\n",
    "                  cleaned[s2:e2 + 1] = 0\n",
    "\n",
    "              # Remove the deleted segment from the list\n",
    "              segments.pop(i if len1 < len2 else i + 1)\n",
    "          else:\n",
    "              i += 1\n",
    "\n",
    "\n",
    "    return cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb0ce5d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.307569Z",
     "iopub.status.busy": "2025-05-27T09:15:46.307374Z",
     "iopub.status.idle": "2025-05-27T09:15:46.321433Z",
     "shell.execute_reply": "2025-05-27T09:15:46.320914Z"
    },
    "id": "2GFazaNobuRe",
    "papermill": {
     "duration": 0.021757,
     "end_time": "2025-05-27T09:15:46.322579",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.300822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm import tqdm\n",
    "def fix_before_P(signal,mask,p_start,p_end,slope_threshold=0.02):\n",
    "  diff_signal = np.diff([signal[p_start],signal[p_start-5]])  # check around the Q point for slope change\n",
    "  j = 0\n",
    "  while True:\n",
    "    while np.abs(diff_signal[-1]) >= slope_threshold and signal[p_start]>signal[p_start-1] and mask[p_start-1]==0:  # Continue until slope becomes small\n",
    "        j+=1\n",
    "        p_start -= 1\n",
    "        mask[p_start] = 1  # mark as part of the QRS\n",
    "        diff_signal = np.diff([signal[p_start],signal[p_start-5]])  # re-evaluate slope\n",
    "    if mask[p_start-1]!=0:\n",
    "      break\n",
    "    if j == 0:\n",
    "\n",
    "        slope_threshold -= 0.001\n",
    "        #print(\"slope_threshold: \",slope_threshold)\n",
    "        if slope_threshold < 0.005:\n",
    "          break\n",
    "    else:\n",
    "        break\n",
    "  #print (\"fixed_before with peak:\",j)\n",
    "  return mask,p_start\n",
    "\n",
    "\n",
    "def fix_P(signal, mask):\n",
    "    p_mask = (mask == 1).astype(int)\n",
    "    qrs_mask = (mask == 2).astype(int)\n",
    "\n",
    "    transitions = np.diff(p_mask, prepend=0)\n",
    "    p_starts = np.where(transitions == 1)[0]\n",
    "\n",
    "    qrs_starts = np.where(np.diff(qrs_mask, prepend=0) == 1)[0]\n",
    "\n",
    "    fixed_p_info = []\n",
    "\n",
    "    for i in tqdm(range(len(p_starts)), desc=\"Processing P\"):\n",
    "\n",
    "        slope_threshold = 0.02  # arbitrary threshold for slope to be considered small\n",
    "        p_start = p_starts[i]\n",
    "\n",
    "        p_next = p_starts[i+1] if i < len(p_starts) - 1 else len(mask)\n",
    "        p_indices = np.where((mask == 1) & (np.arange(len(mask)) >= p_start) & (np.arange(len(mask)) < p_next))[0]\n",
    "\n",
    "        p_end = p_indices[-1] if  len(p_indices)>0 else p_start\n",
    "        #print(\"p_start,p_end: \",p_start,p_end)\n",
    "        # Get indices of current P segment\n",
    "        if len(p_indices) < 3:\n",
    "            continue\n",
    "\n",
    "        p_wave = signal[p_indices]\n",
    "\n",
    "        # Check for peak inside current P segment\n",
    "        peaks, _ = find_peaks(p_wave, prominence=0.01)\n",
    "        has_peak = len(peaks) > 0\n",
    "        peak_index = p_indices[peaks[0]] if has_peak else None\n",
    "\n",
    "        # If no peak, look after end of P segment\n",
    "        post_p_peak_index = None\n",
    "        if has_peak:\n",
    "          #print(\"has peak\")\n",
    "          mask, p_start = fix_before_P(signal, mask,p_start,p_end)\n",
    "          j=0\n",
    "          #print(p_end)\n",
    "          while (p_end + 1 < len(signal)) and signal[p_end] > signal[p_start] and mask[p_end + 1] == 0:\n",
    "            j += 1\n",
    "            p_end += 1\n",
    "            mask[p_end] = 1\n",
    "        if not has_peak:\n",
    "\n",
    "            # Look ahead to the next QRS start\n",
    "            next_qrs_start = qrs_starts[qrs_starts > p_end]\n",
    "            next_qrs_start = next_qrs_start[0] if len(next_qrs_start) > 0 else len(signal)\n",
    "\n",
    "            # Look AFTER the P segment\n",
    "            post_range = np.arange(p_start,min(len(signal) ,p_end + 50,  next_qrs_start ))\n",
    "            post_peaks = []\n",
    "            if len(post_range) > 3:\n",
    "                post_wave = signal[post_range]\n",
    "                peaks, _ = find_peaks(post_wave, prominence=0.01)\n",
    "                # Filter by mask == 0\n",
    "                for p in peaks:\n",
    "                    peak_idx = post_range[p]\n",
    "                    # Check that the region from p_end to peak_idx is all mask == 0\n",
    "                    if np.all(mask[p_end+1:peak_idx + 1] == 0):\n",
    "                        post_peaks.append(peak_idx)\n",
    "\n",
    "\n",
    "            # Look BEFORE the P segment\n",
    "            pre_range = np.arange(max(0, p_start - 50), p_end)  # limit the look-back window to ~400ms\n",
    "            pre_peaks = []\n",
    "            if len(pre_range) > 3:\n",
    "                pre_wave = signal[pre_range]\n",
    "                peaks, _ = find_peaks(pre_wave, prominence=0.01)\n",
    "                # Filter by mask == 0\n",
    "                pre_peaks = [pre_range[p] for p in peaks if mask[pre_range[p]] == 0]\n",
    "                for p in peaks:\n",
    "                    peak_idx = pre_range[p]\n",
    "                    # Check that the region from p_end to peak_idx is all mask == 0\n",
    "                    if np.all(mask[peak_idx + 1:p_start] == 0):\n",
    "                        pre_peaks.append(peak_idx)\n",
    "\n",
    "            # Closest peak\n",
    "            ## Combine both and choose closest properly\n",
    "            closest_peak = None\n",
    "            min_distance = float('inf')\n",
    "\n",
    "            ## Compare post-peaks to p_end\n",
    "            for peak in post_peaks:\n",
    "                dist = abs(peak - p_end)\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    closest_peak = peak\n",
    "\n",
    "            ## Compare pre-peaks to p_start\n",
    "            for peak in pre_peaks:\n",
    "                dist = abs(peak - p_start)\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    closest_peak = peak\n",
    "\n",
    "            post_p_peak_index = closest_peak if closest_peak is not None else None\n",
    "\n",
    "            peak = None\n",
    "            if post_p_peak_index is not None:\n",
    "              if post_p_peak_index < p_start:\n",
    "                peak = \"before\"\n",
    "              else:\n",
    "                peak = \"after\"\n",
    "\n",
    "\n",
    "            #print(peak)\n",
    "            if peak == \"after\":\n",
    "                mask, p_start = fix_before_P(signal, mask,p_start,p_end)\n",
    "                j=0\n",
    "                while (p_end + 1 < len(signal)) and signal[p_end] > signal[p_start] and mask[p_end + 1] == 0:\n",
    "                  j += 1\n",
    "                  p_end += 1\n",
    "                  mask[p_end] = 1\n",
    "            elif peak == \"before\":\n",
    "\n",
    "                mask[post_p_peak_index-2:p_start] = 1\n",
    "                p_start = post_p_peak_index-2\n",
    "                mask, p_start = fix_before_P(signal, mask,p_start,p_end)\n",
    "\n",
    "\n",
    "        fixed_p_info.append({\n",
    "            'start': p_indices[0],\n",
    "            'end': p_indices[-1],\n",
    "            'has_peak': has_peak,\n",
    "            'peak_index': peak_index,\n",
    "            'post_p_peak_index': post_p_peak_index\n",
    "        })\n",
    "        #print (fixed_p_info)\n",
    "\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "419edcc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.334839Z",
     "iopub.status.busy": "2025-05-27T09:15:46.334640Z",
     "iopub.status.idle": "2025-05-27T09:15:46.424876Z",
     "shell.execute_reply": "2025-05-27T09:15:46.424307Z"
    },
    "papermill": {
     "duration": 0.097624,
     "end_time": "2025-05-27T09:15:46.426125",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.328501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fast_fix_QRS(signal, mask, fs=250):\n",
    "    # time = np.arange(len(signal)) / fs\n",
    "    # indices = np.arange(len(mask))  # Precompute indices array\n",
    "\n",
    "    # Precompute slopes for QRS start and end adjustments\n",
    "    # slope_start = np.zeros_like(signal)\n",
    "    # slope_start[3:] = signal[3:] - signal[:-3]  # slope[i] = signal[i] - signal[i-3]\n",
    "    slope_end = np.zeros_like(signal)\n",
    "    slope_end[:-5] = signal[:-5] - signal[5:]    # slope[i] = signal[i] - signal[i+5]\n",
    "\n",
    "    # Identify QRS regions (mask == 2)\n",
    "    qrs_mask = (mask == 2).astype(np.int8)\n",
    "    transitions = np.diff(qrs_mask, prepend=0)\n",
    "    qrs_starts = np.flatnonzero(transitions == 1)\n",
    "    qrs_ends = np.flatnonzero(transitions == -1) - 1\n",
    "    n = len(signal)\n",
    "\n",
    "    if len(qrs_ends) < len(qrs_starts):\n",
    "        qrs_ends = np.append(qrs_ends, n - 1)\n",
    "\n",
    "    for i in tqdm(range(len(qrs_starts)), desc=\"Processing QRS\"):\n",
    "        qrs_start = qrs_starts[i]\n",
    "        # next_qrs_start = qrs_starts[i+1] if i < len(qrs_starts)-1 else len(mask)\n",
    "        # Find QRS end within the current segment\n",
    "        qrs_end = qrs_ends[i]\n",
    "        #before_qrs_end = qrs_ends[i-1] if i > 0 else 0\n",
    "        # Adjust QRS start based on preceding P wave\n",
    "        p_indices = np.where(mask[max(0, qrs_start-200):qrs_start] == 1)[0]\n",
    "        valid_p = []\n",
    "        if len(p_indices) > 0:\n",
    "            # Check from the end backwards\n",
    "            p_end = p_indices[-1]\n",
    "            p_end = p_indices[-1]  # just take the last one\n",
    "            if not np.any(mask[p_end:qrs_start] >= 2):  # no QRS or T in between\n",
    "                p_start = p_end\n",
    "                if p_start > 0 and mask[p_start-1] == 1:\n",
    "                    p_start -= 1\n",
    "                valid_p = np.arange(p_start, p_end + 1)\n",
    "                \n",
    "        if len(valid_p) > 0 and (qrs_start - valid_p[-1]) < 20:\n",
    "            # print(\"vaid p, qrs moved back:\",qrs_start-p_end)\n",
    "            mask[p_end+1:qrs_start] = 2\n",
    "            qrs_start = p_end +1\n",
    "        else:\n",
    "            # Precompute signal range before QRS\n",
    "            # print(\"no vaid p\")\n",
    "            \n",
    "            pre_start = max(0, qrs_start - 100)\n",
    "            pre_wave = signal[pre_start:qrs_start]\n",
    "            \n",
    "            # Find first valid peak before QRS\n",
    "            first_peak_before_qrs = None\n",
    "            if len(pre_wave) > 3:\n",
    "                peaks, _ = find_peaks(pre_wave, prominence=0.01)\n",
    "                for p in peaks:\n",
    "                    if mask[pre_start + p] == 0:\n",
    "                        # print(\"found peak before qrs\")\n",
    "                        first_peak_before_qrs = pre_start + p\n",
    "                        break\n",
    "            # Track first peak (if any)\n",
    "\n",
    "                    \n",
    "            original_qrs_start = qrs_start  # save original position\n",
    "            slope_threshold = 0.02\n",
    "            max_back_steps = 25\n",
    "            back_steps = 0\n",
    "        \n",
    "            while back_steps < max_back_steps and qrs_start >= 3:\n",
    "                       \n",
    "                diff_signal = signal[qrs_start] - signal[qrs_start - 3]\n",
    "                if abs(diff_signal) < slope_threshold:\n",
    "                    \n",
    "                    # Dynamic threshold (only if stuck)\n",
    "                    if back_steps == 0 and slope_threshold>0.005:\n",
    "                        slope_threshold = max(slope_threshold - 0.001, 0.005)\n",
    "                        continue \n",
    "                    break\n",
    "        \n",
    "                if first_peak_before_qrs is not None and qrs_start <= first_peak_before_qrs:\n",
    "                    break\n",
    "        \n",
    "                if mask[qrs_start - 1] != 0:\n",
    "                    break\n",
    "        \n",
    "                # Move QRS start backward\n",
    "                qrs_start -= 1\n",
    "                back_steps += 1\n",
    "        \n",
    "            \n",
    "            mask[qrs_start:original_qrs_start] = 2\n",
    "            # print(\"vaid p, qrs moved back:\",original_qrs_start-qrs_start)\n",
    "            \n",
    "        # Adjust QRS end\n",
    "        # Extend until signal stops descending\n",
    "        original_qrs_end = qrs_end\n",
    "        while qrs_end < len(signal)-1 and mask[qrs_end+1] == 0 and signal[qrs_end] >= signal[qrs_end+1]:\n",
    "            qrs_end += 1\n",
    "        \n",
    "        # Vectorize the mask update to do it all at once\n",
    "        mask[original_qrs_end:qrs_end+1] = 2\n",
    "\n",
    "        # Further adjust based on slope\n",
    "        slope_threshold_end = 0.02\n",
    "        max_forward_steps = 25\n",
    "        forward_steps = 0\n",
    "        # print(\"vaid p, qrs moved back:\",original_qrs_start-qrs_start)\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            while (qrs_end < len(signal)-5 \n",
    "                   and forward_steps < max_forward_steps \n",
    "                   and abs(slope_end[qrs_end]) >= slope_threshold_end \n",
    "                   and mask[qrs_end+1] == 0):\n",
    "                qrs_end += 1\n",
    "                forward_steps += 1\n",
    "            if abs(slope_end[qrs_end]) < slope_threshold_end and forward_steps == 0 and slope_threshold_end>0.005: \n",
    "                slope_threshold_end = max(slope_threshold_end - 0.001, 0.005)\n",
    "                continue\n",
    "            else :\n",
    "                mask[qrs_end - forward_steps + 1: qrs_end + 1] = 2\n",
    "                break\n",
    "                                \n",
    "            \n",
    "        \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99f121d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.438898Z",
     "iopub.status.busy": "2025-05-27T09:15:46.438681Z",
     "iopub.status.idle": "2025-05-27T09:15:46.444040Z",
     "shell.execute_reply": "2025-05-27T09:15:46.443217Z"
    },
    "id": "b8dM91RXhegF",
    "outputId": "251c71b8-b899-4008-b25b-cde66a81fdc2",
    "papermill": {
     "duration": 0.012907,
     "end_time": "2025-05-27T09:15:46.445199",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.432292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 2 0 3 3 3 3 3 3 3 3 3 3 3 0 0 0 1 1 0\n",
      " 0 2 2 0 3 3 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def post_process_ecg(predicted):\n",
    "\n",
    "    predicted = remove_uncomplete_first_last_wave(predicted)\n",
    "    predicted = merge_close_waves(predicted)\n",
    "    predicted = remove_irrelevant_waves(predicted)\n",
    "    predicted = check_repeated_waves(predicted)\n",
    "\n",
    "    return predicted\n",
    "\n",
    "predicted = np.array([1,1,0, 1, 1, 0,0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 2, 0, 3, 3, 0,0,0,0,0,0,3, 3, 3, 0,0,0,1, 1, 0, 0, 2, 2, 0, 3, 3,0,3,3])\n",
    "predicted = post_process_ecg(predicted)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c65e9",
   "metadata": {
    "id": "IsKW-178f8s7",
    "papermill": {
     "duration": 0.005481,
     "end_time": "2025-05-27T09:15:46.456880",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.451399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbb98ca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:46.469276Z",
     "iopub.status.busy": "2025-05-27T09:15:46.469072Z",
     "iopub.status.idle": "2025-05-27T09:15:50.980143Z",
     "shell.execute_reply": "2025-05-27T09:15:50.979322Z"
    },
    "id": "a07tScN96T4a",
    "papermill": {
     "duration": 4.518963,
     "end_time": "2025-05-27T09:15:50.981604",
     "exception": false,
     "start_time": "2025-05-27T09:15:46.462641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualConvBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dilation=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3,\n",
    "                              padding=dilation, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3,\n",
    "                               padding=dilation, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        return F.relu(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv1d(F_g, F_int, kernel_size=1),\n",
    "            nn.BatchNorm1d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv1d(F_l, F_int, kernel_size=1),\n",
    "            nn.BatchNorm1d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv1d(F_int, 1, kernel_size=1),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class UNet1D_Enhanced(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_classes=4, base_filters=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder with residual blocks and multi-scale context\n",
    "        self.enc1 = ResidualConvBlock1D(in_channels, base_filters)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.enc2 = ResidualConvBlock1D(base_filters, base_filters*2)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.enc3 = ResidualConvBlock1D(base_filters*2, base_filters*4)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.enc4 = ResidualConvBlock1D(base_filters*4, base_filters*8)\n",
    "        self.pool4 = nn.MaxPool1d(2)\n",
    "\n",
    "        # Bottleneck with dilated convolution\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ResidualConvBlock1D(base_filters*8, base_filters*16, dilation=2),\n",
    "            ResidualConvBlock1D(base_filters*16, base_filters*16, dilation=4)\n",
    "        )\n",
    "\n",
    "        # Decoder with attention gates\n",
    "        self.upconv4 = nn.ConvTranspose1d(base_filters*16, base_filters*8, kernel_size=2, stride=2)\n",
    "        self.att4 = AttentionBlock(base_filters*8, base_filters*8, base_filters//2)\n",
    "        self.dec4 = ResidualConvBlock1D(base_filters*16, base_filters*8)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose1d(base_filters*8, base_filters*4, kernel_size=2, stride=2)\n",
    "        self.att3 = AttentionBlock(base_filters*4, base_filters*4, base_filters//4)\n",
    "        self.dec3 = ResidualConvBlock1D(base_filters*8, base_filters*4)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose1d(base_filters*4, base_filters*2, kernel_size=2, stride=2)\n",
    "        self.att2 = AttentionBlock(base_filters*2, base_filters*2, base_filters//8)\n",
    "        self.dec2 = ResidualConvBlock1D(base_filters*4, base_filters*2)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose1d(base_filters*2, base_filters, kernel_size=2, stride=2)\n",
    "        self.att1 = AttentionBlock(base_filters, base_filters, base_filters//16)\n",
    "        self.dec1 = ResidualConvBlock1D(base_filters*2, base_filters)\n",
    "\n",
    "        # Output with multi-scale feature fusion\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv1d(base_filters, base_filters//2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(base_filters//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(base_filters//2, n_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        # Boundary refinement\n",
    "        self.boundary_refine = nn.Conv1d(n_classes, n_classes, kernel_size=5, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        e4 = self.enc4(self.pool3(e3))\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool4(e4))\n",
    "\n",
    "        # Decoder with attention\n",
    "        d4 = self.upconv4(b)\n",
    "        e4 = self.att4(d4, center_crop(e4, d4.shape[-1]))\n",
    "        d4 = self.dec4(torch.cat([d4, e4], dim=1))\n",
    "\n",
    "        d3 = self.upconv3(d4)\n",
    "        e3 = self.att3(d3, center_crop(e3, d3.shape[-1]))\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        e2 = self.att2(d2, center_crop(e2, d2.shape[-1]))\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        e1 = self.att1(d1, center_crop(e1, d1.shape[-1]))\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        # Output\n",
    "        out = self.final_conv(d1)\n",
    "\n",
    "        # Boundary refinement\n",
    "        out = out + self.boundary_refine(out)  # Residual connection\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def center_crop(enc_feat, target_size):\n",
    "    _, _, L = enc_feat.size()\n",
    "    diff = L - target_size\n",
    "    if diff == 0:\n",
    "        return enc_feat\n",
    "    elif diff < 0:\n",
    "        raise ValueError(\"Encoder feature shorter than target\")\n",
    "    start = diff // 2\n",
    "    end = start + target_size\n",
    "    return enc_feat[:, :, start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a613fe91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:50.996080Z",
     "iopub.status.busy": "2025-05-27T09:15:50.995404Z",
     "iopub.status.idle": "2025-05-27T09:15:52.384038Z",
     "shell.execute_reply": "2025-05-27T09:15:52.383267Z"
    },
    "id": "Kjq9YZXZQbLl",
    "outputId": "3d9aecce-4247-4762-cfcb-817d4398b7a7",
    "papermill": {
     "duration": 1.39728,
     "end_time": "2025-05-27T09:15:52.385305",
     "exception": false,
     "start_time": "2025-05-27T09:15:50.988025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/2491763897.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_deep.load_state_dict(torch.load(\"/kaggle/input/unet-model-for-ecg-mask-detection/pytorch/default/1/updated_unet1d_ecg_qrs.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet1D_Enhanced(\n",
       "  (enc1): ResidualConvBlock1D(\n",
       "    (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1d(1, 64, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc2): ResidualConvBlock1D(\n",
       "    (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc3): ResidualConvBlock1D(\n",
       "    (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc4): ResidualConvBlock1D(\n",
       "    (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): Sequential(\n",
       "    (0): ResidualConvBlock1D(\n",
       "      (conv1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "      (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "      (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualConvBlock1D(\n",
       "      (conv1): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "      (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "      (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (upconv4): ConvTranspose1d(1024, 512, kernel_size=(2,), stride=(2,))\n",
       "  (att4): AttentionBlock(\n",
       "    (W_g): Sequential(\n",
       "      (0): Conv1d(512, 32, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (W_x): Sequential(\n",
       "      (0): Conv1d(512, 32, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (psi): Sequential(\n",
       "      (0): Conv1d(32, 1, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Sigmoid()\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec4): ResidualConvBlock1D(\n",
       "    (conv1): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (upconv3): ConvTranspose1d(512, 256, kernel_size=(2,), stride=(2,))\n",
       "  (att3): AttentionBlock(\n",
       "    (W_g): Sequential(\n",
       "      (0): Conv1d(256, 16, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (W_x): Sequential(\n",
       "      (0): Conv1d(256, 16, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (psi): Sequential(\n",
       "      (0): Conv1d(16, 1, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Sigmoid()\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec3): ResidualConvBlock1D(\n",
       "    (conv1): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (upconv2): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))\n",
       "  (att2): AttentionBlock(\n",
       "    (W_g): Sequential(\n",
       "      (0): Conv1d(128, 8, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (W_x): Sequential(\n",
       "      (0): Conv1d(128, 8, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (psi): Sequential(\n",
       "      (0): Conv1d(8, 1, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Sigmoid()\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec2): ResidualConvBlock1D(\n",
       "    (conv1): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (upconv1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))\n",
       "  (att1): AttentionBlock(\n",
       "    (W_g): Sequential(\n",
       "      (0): Conv1d(64, 4, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (W_x): Sequential(\n",
       "      (0): Conv1d(64, 4, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (psi): Sequential(\n",
       "      (0): Conv1d(4, 1, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Sigmoid()\n",
       "    )\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (dec1): ResidualConvBlock1D(\n",
       "    (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Sequential(\n",
       "    (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(32, 4, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (boundary_refine): Conv1d(4, 4, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize model\n",
    "model_deep = UNet1D_Enhanced(n_classes=4).to(device)\n",
    "model_deep.load_state_dict(torch.load(\"/kaggle/input/unet-model-for-ecg-mask-detection/pytorch/default/1/updated_unet1d_ecg_qrs.pth\", map_location=device))\n",
    "model_deep.to(device)\n",
    "model_deep.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da14cc7",
   "metadata": {
    "id": "uWCRAJaggYfb",
    "papermill": {
     "duration": 0.005869,
     "end_time": "2025-05-27T09:15:52.397630",
     "exception": false,
     "start_time": "2025-05-27T09:15:52.391761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a2bc2",
   "metadata": {
    "id": "7l6N9wIkOUkH",
    "papermill": {
     "duration": 0.005827,
     "end_time": "2025-05-27T09:15:52.409250",
     "exception": false,
     "start_time": "2025-05-27T09:15:52.403423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# making dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4445936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:52.421975Z",
     "iopub.status.busy": "2025-05-27T09:15:52.421531Z",
     "iopub.status.idle": "2025-05-27T09:15:52.427869Z",
     "shell.execute_reply": "2025-05-27T09:15:52.427196Z"
    },
    "id": "RxQOkRn132tk",
    "papermill": {
     "duration": 0.014052,
     "end_time": "2025-05-27T09:15:52.429142",
     "exception": false,
     "start_time": "2025-05-27T09:15:52.415090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_and_save_mask(record_name, path, model, start=None, end=None, target_fs=250, save_path=\"masks/\"):\n",
    "    record = wfdb.rdrecord(path)\n",
    "    signal = record.p_signal[:, 0]  # lead I\n",
    "    fs = record.fs\n",
    "\n",
    "    if end is None:\n",
    "        end = len(signal) / fs\n",
    "    if start is None:\n",
    "        start = 0\n",
    "    signal = signal[int(start*fs):int(end*fs)]\n",
    "\n",
    "    if fs != 250:\n",
    "        signal = resample_signal(signal, original_fs=fs, target_fs=target_fs)\n",
    "        fs = target_fs\n",
    "\n",
    "    # Predict\n",
    "    pred_labels, segments, _ = predict_full_ecg(signal, model, fs=fs)\n",
    "    signal = np.concatenate(segments, axis=2).reshape(-1)\n",
    "\n",
    "    # Post-process mask\n",
    "    pred_labels = remove_uncomplete_first_last_wave(pred_labels)\n",
    "    pred_labels = merge_close_waves(pred_labels)\n",
    "    pred_labels = remove_irrelevant_waves(pred_labels)\n",
    "    pred_labels = check_repeated_waves(pred_labels)\n",
    "    pred_labels = fix_P(signal, pred_labels)\n",
    "    pred_labels = fast_fix_QRS(signal, pred_labels, fs=fs)\n",
    "    pred_labels = merge_close_waves(pred_labels)\n",
    "\n",
    "    # Save mask\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    mask_filename = os.path.join(save_path, f\"{record_name}_mask.csv\")\n",
    "\n",
    "    # Save the file\n",
    "    os.makedirs(os.path.dirname(mask_filename), exist_ok=True)\n",
    "    np.savetxt(mask_filename, pred_labels, delimiter=\",\", fmt=\"%d\")\n",
    "    print(f\"Saved mask to: {mask_filename}\")\n",
    "    return mask_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d1283fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:52.442092Z",
     "iopub.status.busy": "2025-05-27T09:15:52.441530Z",
     "iopub.status.idle": "2025-05-27T09:15:52.447235Z",
     "shell.execute_reply": "2025-05-27T09:15:52.446549Z"
    },
    "id": "Vp0n6IPwN60K",
    "papermill": {
     "duration": 0.013254,
     "end_time": "2025-05-27T09:15:52.448339",
     "exception": false,
     "start_time": "2025-05-27T09:15:52.435085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import contextlib\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        sys.stdout = devnull\n",
    "        sys.stderr = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr\n",
    "\n",
    "# Patch tqdm globally if needed\n",
    "silent_tqdm = partial(tqdm, disable=True)\n",
    "\n",
    "def batch_predict_masks(mitdb_path, mit_records, model, target_fs=250):\n",
    "    \n",
    "    all_dfs = []\n",
    "\n",
    "    #for record_name in tqdm(mit_records, total=len(mit_records), desc=\"Processing mit_records\"):\n",
    "    for record_name in tqdm( mit_records, total=len(mit_records), desc=\"mask detection\"):\n",
    "        full_path = os.path.join(mitdb_path, record_name)\n",
    "        with suppress_output():\n",
    "            df = predict_and_save_mask(str(record_name), full_path, model, target_fs=250)   \n",
    "            \n",
    "\n",
    "    return all_dfs\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05c199b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:52.461039Z",
     "iopub.status.busy": "2025-05-27T09:15:52.460656Z",
     "iopub.status.idle": "2025-05-27T09:15:53.049677Z",
     "shell.execute_reply": "2025-05-27T09:15:53.048663Z"
    },
    "papermill": {
     "duration": 0.596926,
     "end_time": "2025-05-27T09:15:53.051063",
     "exception": false,
     "start_time": "2025-05-27T09:15:52.454137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-27 09:15:52--  https://physionet.org/files/challenge-2021/1.0.3/RECORDS\r\n",
      "Resolving physionet.org (physionet.org)... 18.18.42.54\r\n",
      "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 2073 (2.0K) [text/plain]\r\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/RECORDS’\r\n",
      "\r\n",
      "physionet.org/files 100%[===================>]   2.02K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2025-05-27 09:15:52 (916 MB/s) - ‘physionet.org/files/challenge-2021/1.0.3/RECORDS’ saved [2073/2073]\r\n",
      "\r\n",
      "FINISHED --2025-05-27 09:15:52--\r\n",
      "Total wall clock time: 0.3s\r\n",
      "Downloaded: 1 files, 2.0K in 0s (916 MB/s)\r\n"
     ]
    }
   ],
   "source": [
    "!wget -r -N -c -np https://physionet.org/files/challenge-2021/1.0.3/RECORDS\n",
    "with open('/kaggle/working/physionet.org/files/challenge-2021/1.0.3/RECORDS', 'r') as file:\n",
    "    folder_paths = [line.strip() for line in file]\n",
    "!rm -rf /kaggle/working/physionet.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2716c9b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:53.065434Z",
     "iopub.status.busy": "2025-05-27T09:15:53.064885Z",
     "iopub.status.idle": "2025-05-27T09:15:55.707621Z",
     "shell.execute_reply": "2025-05-27T09:15:55.706657Z"
    },
    "papermill": {
     "duration": 2.65144,
     "end_time": "2025-05-27T09:15:55.709001",
     "exception": false,
     "start_time": "2025-05-27T09:15:53.057561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88253\n"
     ]
    }
   ],
   "source": [
    "#mit_records = ['123']\n",
    "import os\n",
    "import random\n",
    "\n",
    "def get_all_records(directory, prefix=\"\", extension=\".hea\"):\n",
    "    # Remove extension and ensure unique filenames\n",
    "    #print(directory)\n",
    "    files = [f.replace(extension, \"\") for f in os.listdir(directory) if f.endswith(extension)]\n",
    "    unique_files = list(set(files))\n",
    "    \n",
    "    # Add the prefix to each filename\n",
    "    full_paths = [os.path.join(prefix, f) for f in unique_files]\n",
    "    \n",
    "    return full_paths\n",
    "\n",
    "base_path = \"/kaggle/input/signal-classification-data/challenge-2021/\"\n",
    "full_records = []\n",
    "for path in folder_paths:\n",
    "    full_records += get_all_records(base_path+path,path)\n",
    "\n",
    "#full_records.sort()\n",
    "random.shuffle(full_records)\n",
    "print(len(full_records))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8310ab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:55.723443Z",
     "iopub.status.busy": "2025-05-27T09:15:55.722895Z",
     "iopub.status.idle": "2025-05-27T09:15:55.854857Z",
     "shell.execute_reply": "2025-05-27T09:15:55.853829Z"
    },
    "papermill": {
     "duration": 0.140505,
     "end_time": "2025-05-27T09:15:55.856262",
     "exception": false,
     "start_time": "2025-05-27T09:15:55.715757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b1128c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T09:15:55.870643Z",
     "iopub.status.busy": "2025-05-27T09:15:55.870030Z",
     "iopub.status.idle": "2025-05-27T12:11:44.217508Z",
     "shell.execute_reply": "2025-05-27T12:11:44.216767Z"
    },
    "id": "D-S5WHnjN70v",
    "outputId": "0e67fd46-3a81-4818-c0a6-ecb61462fb76",
    "papermill": {
     "duration": 10548.356005,
     "end_time": "2025-05-27T12:11:44.218817",
     "exception": false,
     "start_time": "2025-05-27T09:15:55.862812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mask detection: 100%|██████████| 88253/88253 [2:55:48<00:00,  8.37it/s]\n"
     ]
    }
   ],
   "source": [
    "#nb = len(df_descreption['ecg_id'])\n",
    "df_all = batch_predict_masks(base_path, full_records, model_deep )\n",
    "# 11208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4bb908e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:11:48.354893Z",
     "iopub.status.busy": "2025-05-27T12:11:48.354212Z",
     "iopub.status.idle": "2025-05-27T12:11:48.359833Z",
     "shell.execute_reply": "2025-05-27T12:11:48.359317Z"
    },
    "papermill": {
     "duration": 2.044833,
     "end_time": "2025-05-27T12:11:48.361017",
     "exception": false,
     "start_time": "2025-05-27T12:11:46.316184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef extract_features_from_mask(record_name, path, mask_path, fs=250):\\n    print(\\'start\\')\\n    record = wfdb.rdrecord(path)\\n    signal = record.p_signal[:, 0]\\n\\n    mask = np.loadtxt(mask_path, delimiter=\",\", dtype=int)\\n\\n    boundaries = get_wave_boundaries(mask)\\n    all_predicted_masks = mask.reshape(1, -1)\\n    signal = signal[:len(mask)]  # crop if needed\\n\\n    features_per_beat = extract_features_per_qrs(signal, all_predicted_masks.flatten(), fs)\\n\\n    df = pd.DataFrame(features_per_beat)\\n    if df.empty:\\n        return df\\n\\n    df = df[[\\'R_index\\'] + [col for col in df.columns if col != \\'R_index\\']]\\n    y_pred = Type_model.predict(df)\\n\\n    for i, feature_dict in enumerate(features_per_beat):\\n        Type = class_map[int(y_pred[i])]\\n        feature_dict[\"Type\"] = Type\\n\\n    df = pd.DataFrame(features_per_beat)\\n    df[\\'record_name\\'] = record_name\\n    df[\\'beat_number\\'] = range(1, len(df) + 1)\\n\\n    main_cols = [\\'record_name\\', \\'beat_number\\', \\'Type\\', \\'R_index\\']\\n    df = df[main_cols + [col for col in df.columns if col not in main_cols]]\\n    print(\\'end\\')\\n    \\n    return df\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def extract_features_from_mask(record_name, path, mask_path, fs=250):\n",
    "    print('start')\n",
    "    record = wfdb.rdrecord(path)\n",
    "    signal = record.p_signal[:, 0]\n",
    "\n",
    "    mask = np.loadtxt(mask_path, delimiter=\",\", dtype=int)\n",
    "\n",
    "    boundaries = get_wave_boundaries(mask)\n",
    "    all_predicted_masks = mask.reshape(1, -1)\n",
    "    signal = signal[:len(mask)]  # crop if needed\n",
    "\n",
    "    features_per_beat = extract_features_per_qrs(signal, all_predicted_masks.flatten(), fs)\n",
    "\n",
    "    df = pd.DataFrame(features_per_beat)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df = df[['R_index'] + [col for col in df.columns if col != 'R_index']]\n",
    "    y_pred = Type_model.predict(df)\n",
    "\n",
    "    for i, feature_dict in enumerate(features_per_beat):\n",
    "        Type = class_map[int(y_pred[i])]\n",
    "        feature_dict[\"Type\"] = Type\n",
    "\n",
    "    df = pd.DataFrame(features_per_beat)\n",
    "    df['record_name'] = record_name\n",
    "    df['beat_number'] = range(1, len(df) + 1)\n",
    "\n",
    "    main_cols = ['record_name', 'beat_number', 'Type', 'R_index']\n",
    "    df = df[main_cols + [col for col in df.columns if col not in main_cols]]\n",
    "    print('end')\n",
    "    \n",
    "    return df\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cad63231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T12:11:52.573058Z",
     "iopub.status.busy": "2025-05-27T12:11:52.572777Z",
     "iopub.status.idle": "2025-05-27T12:11:52.577961Z",
     "shell.execute_reply": "2025-05-27T12:11:52.577409Z"
    },
    "papermill": {
     "duration": 2.162781,
     "end_time": "2025-05-27T12:11:52.578895",
     "exception": false,
     "start_time": "2025-05-27T12:11:50.416114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport wfdb\\nimport pandas as pd\\nimport contextlib\\nfrom functools import partial\\nimport sys\\n\\n\\ndef batch_features(mitdb_path, mit_records, model, start=None, end=None, target_fs=250, save_csv=False, output_csv=\"all_features.csv\"):\\n    \\n    all_dfs = []\\n    print(\\'mit_records,\\',mit_records)\\n    #for record_name in tqdm(mit_records, total=len(mit_records), desc=\"Processing mit_records\"):\\n    for record_name in mit_records:\\n        print(\"record_name,\",record_name)\\n        full_path = os.path.join(mitdb_path, record_name)\\n        chunk_duration_sec = 10  # 30 minutes\\n        record = wfdb.rdrecord(full_path)\\n        #record = wfdb.rdrecord(edb_path+\\'e0129\\')# Run prediction\\n\\n        signal = record.p_signal[:, 0]  # lead I\\n\\n        fs = record.fs\\n        total_duration_sec = len(record.p_signal[:, 0]) / record.fs\\n        \\n        for start_time in range(0, int(total_duration_sec), chunk_duration_sec):\\n            end_time = min(start_time + chunk_duration_sec, total_duration_sec)\\n            #with suppress_output():\\n            print(\"record:\",record_name,\" from: \",start_time,\"to: \",end_time)\\n            name = str(record_name)+\"_\"+str(start_time)+\"_\"+str(end_time)\\n            df = extract_features_from_mask(name, full_path, \\'/kaggle/working/masks/\\'+name+\\'_mask.csv\\', fs=250)   \\n            \\n            all_dfs.append(df)\\n\\n    combined_df = pd.concat(all_dfs, ignore_index=True)\\n\\n    if save_csv:\\n        combined_df.to_csv(output_csv, index=False)\\n        print(f\"Saved combined features to {output_csv}\")\\n\\n    return combined_df\\n\\ndf_all = batch_features(mitdb_path, mit_records[:1], model_deep , save_csv=True,output_csv=\"mitdb_norm_features.csv\") \\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import contextlib\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "\n",
    "def batch_features(mitdb_path, mit_records, model, start=None, end=None, target_fs=250, save_csv=False, output_csv=\"all_features.csv\"):\n",
    "    \n",
    "    all_dfs = []\n",
    "    print('mit_records,',mit_records)\n",
    "    #for record_name in tqdm(mit_records, total=len(mit_records), desc=\"Processing mit_records\"):\n",
    "    for record_name in mit_records:\n",
    "        print(\"record_name,\",record_name)\n",
    "        full_path = os.path.join(mitdb_path, record_name)\n",
    "        chunk_duration_sec = 10  # 30 minutes\n",
    "        record = wfdb.rdrecord(full_path)\n",
    "        #record = wfdb.rdrecord(edb_path+'e0129')# Run prediction\n",
    "\n",
    "        signal = record.p_signal[:, 0]  # lead I\n",
    "\n",
    "        fs = record.fs\n",
    "        total_duration_sec = len(record.p_signal[:, 0]) / record.fs\n",
    "        \n",
    "        for start_time in range(0, int(total_duration_sec), chunk_duration_sec):\n",
    "            end_time = min(start_time + chunk_duration_sec, total_duration_sec)\n",
    "            #with suppress_output():\n",
    "            print(\"record:\",record_name,\" from: \",start_time,\"to: \",end_time)\n",
    "            name = str(record_name)+\"_\"+str(start_time)+\"_\"+str(end_time)\n",
    "            df = extract_features_from_mask(name, full_path, '/kaggle/working/masks/'+name+'_mask.csv', fs=250)   \n",
    "            \n",
    "            all_dfs.append(df)\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    if save_csv:\n",
    "        combined_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Saved combined features to {output_csv}\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "df_all = batch_features(mitdb_path, mit_records[:1], model_deep , save_csv=True,output_csv=\"mitdb_norm_features.csv\") \n",
    "'''\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8c27c",
   "metadata": {
    "papermill": {
     "duration": 2.046213,
     "end_time": "2025-05-27T12:11:56.654183",
     "exception": false,
     "start_time": "2025-05-27T12:11:54.607970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jvXrP01oftqo",
    "ZW-HQbGIN86a",
    "UCgs5n5KgSse",
    "IsKW-178f8s7"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7418085,
     "sourceId": 11811064,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 238757389,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 355945,
     "modelInstanceId": 334922,
     "sourceId": 410120,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10600.554132,
   "end_time": "2025-05-27T12:12:02.342602",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-27T09:15:21.788470",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
